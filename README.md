# ğŸ“„ RAG-Based Google Doc Chatbot (Gemini + FAISS + Sentence Transformers)

[![Streamlit App](https://static.streamlit.io/badges/streamlit_badge_black_white.svg)](https://policypalassignment-koaszomdwha3zn6kyedziu.streamlit.app/)

A full-stack **Retrieval-Augmented Generation (RAG)** chatbot that automatically ingests content from a publicly shared Google Document and answers user queries with **accurate, citation-grounded responses**.

The system uses **Sentence Transformers for embeddings**, **FAISS for vector search**, and **Google Gemini SDK** for answer generation. It includes a **FastAPI backend**, **Streamlit demo app**, and **HTML/CSS/JS frontend UI**.

---

## ğŸš€ Features

- âœ… Automatic Google Doc ingestion
- âœ… Structured section-aware parsing
- âœ… Semantic chunking with overlap
- âœ… Dense embeddings using Sentence Transformers
- âœ… Fast similarity search using FAISS
- âœ… Gemini-powered grounded answer generation
- âœ… Inline source citations
- âœ… FastAPI backend API
- âœ… Streamlit deployed demo app
- âœ… Web-based frontend UI
- âœ… Config-driven modular pipeline
- âœ… Production-ready architecture

---

## ğŸ— System Architecture

```
Public Google Doc
        â†“
Ingestion Layer
        â†“
Structured Parsing
        â†“
Semantic Chunking
        â†“
Sentence Transformer Embeddings
        â†“
FAISS Vector Index
        â†“
Query Embedding
        â†“
Similarity Search
        â†“
Gemini RAG Generation
        â†“
FastAPI Backend
        â†“
Frontend UI / Streamlit App
```

---

## ğŸ“‚ Project Structure

```
rag-chatbot/
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml
â”‚
â”œâ”€â”€ ingestion/
â”‚   â”œâ”€â”€ fetch_doc.py
â”‚   â”œâ”€â”€ normalize.py
â”‚   â””â”€â”€ run_ingestion.py
â”‚
â”œâ”€â”€ chunking/
â”‚   â”œâ”€â”€ semantic_chunker.py
â”‚   â””â”€â”€ run_chunking.py
â”‚
â”œâ”€â”€ embeddings/
â”‚   â”œâ”€â”€ embedder.py
â”‚   â”œâ”€â”€ vector_store.py
â”‚   â””â”€â”€ run_embedding.py
â”‚
â”œâ”€â”€ retrieval/
â”‚   â””â”€â”€ retriever.py
â”‚
â”œâ”€â”€ rag/
â”‚   â””â”€â”€ rag_pipeline.py
â”‚
â”œâ”€â”€ ui/
â”‚   â”œâ”€â”€ frontend/
â”‚   â”‚   â”œâ”€â”€ index.html
â”‚   â”‚   â”œâ”€â”€ style.css
â”‚   â”‚   â””â”€â”€ app.js
â”‚   â”‚
â”‚   â”œâ”€â”€ backend/
â”‚   â”‚   â””â”€â”€ app.py   (FastAPI)
â”‚   â”‚
â”‚   â””â”€â”€ streamlit_app.py
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ processed/
â”‚   â””â”€â”€ vector_db/
â”‚
â”œâ”€â”€ .env
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âš™ Tech Stack

| Component             | Technology                                |
| --------------------- | ----------------------------------------- |
| LLM                   | Google Gemini (google-genai SDK)          |
| Embeddings            | Sentence Transformers (all-MiniLM-L6-v2)  |
| Vector Database       | FAISS                                     |
| Backend API           | FastAPI                                   |
| Frontend              | HTML, CSS, JavaScript                     |
| Demo App              | Streamlit (Deployed)                      |
| Config                | YAML                                      |
| Environment Variables | python-dotenv                             |
| Hosting               | Streamlit Cloud (Live) / Render (Planned) |

---

## ğŸŒ Live Demo

**Streamlit App:**  
ğŸ‘‰ https://policypalassignment-koaszomdwha3zn6kyedziu.streamlit.app/

---

## ğŸš¦ Deployment Status

- Streamlit Demo: âœ… Live
- FastAPI Backend: ğŸš§ In Progress
- Production Frontend UI: ğŸš§ In Progress

---

## ğŸ”‘ Prerequisites

- Python 3.9+
- Google Gemini API Key
- Public Google Document Link

---

## ğŸ”§ Installation

### 1ï¸âƒ£ Clone Repository

```bash
git clone https://github.com/yourusername/rag-chatbot.git
cd rag-chatbot
```

---

### 2ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

---

### 3ï¸âƒ£ Environment Setup

Create `.env` file:

```env
GEMINI_API_KEY=your_api_key_here
```

---

### 4ï¸âƒ£ Configuration

Edit `config/config.yaml`

```yaml
google_doc_url: "PASTE_PUBLIC_GOOGLE_DOC_LINK"

embedding:
  model_name: all-MiniLM-L6-v2

retrieval:
  top_k: 3

gemini:
  model_name: gemini-2.5-flash
  temperature: 0.2
  max_output_tokens: 1024
```

---

## â–¶ Pipeline Execution

### âœ… Step 1 â€” Document Ingestion

```bash
python -m ingestion.run_ingestion
```

---

### âœ… Step 2 â€” Semantic Chunking

```bash
python -m  chunking.run_chunking
```

---

### âœ… Step 3 â€” Embedding + Index Creation

```bash
python embeddings/run_embedding.py
```

---

### âœ… Step 4 â€” Run Streamlit App

```bash
streamlit run ui/streamlit_app.py
```

---

## ğŸš€ Running FastAPI Backend

### Start API Server

```bash
uvicorn app:app --reload
```

---

### API Base URL

```
http://127.0.0.1:8000
```

---

### Swagger API Docs

```
http://127.0.0.1:8000/docs
```

---

### Example API Request

```json
{
  "query": "What is the leave policy?",
  "chat_history": []
}
```

## ğŸ³ Quick Start with Docker Hub

You can run the PolicyPal backend directly from the pre-built image hosted on Docker Hub.

### 1. Pull and Run the Image

Replace YOUR_API_KEY with your Gemini API Key:

```
docker run -d -p 8000:8000 -e GEMINI_API_KEY="AIzaSyDfL4hqY3oTRKugGn7XHUUAZY1_jNO2rcQ" --name policypal-app bazy07/policy-pal-api:v1

```

### 2. Initialize the Knowledge Base

Since the vector database starts empty, run the following command to ingest the policy documents from the Google Doc into the running container:

```
docker exec -it policypal-app sh -c "python -m ingestion.run_ingestion && python -m chunking.run_chunking && python embeddings/run_embedding.py"
```

### 3. Access the API

```
Backend API: http://localhost:8000
```

### Example API Response

```json
{
  "answer": "Employees are entitled to 12 casual leaves per year.",
  "sources": [
    {
      "section_id": "3.2",
      "title": "Leave Policy",
      "chunk_id": "chunk_14"
    }
  ],
  "timestamp": "2026-01-17T10:12:44"
}
```

---

## âœ… Project Highlights

- End-to-end RAG pipeline implementation
- Google Doc live ingestion system
- Modular scalable architecture
- Production-style backend + frontend separation
- Industry-standard FAISS vector search
- Citation grounded responses

---

## ğŸ“Œ Author

**Pratiksha Kumari**  
AI/ML | Data Science | RAG Systems | NLP Applications

---
